ElasticSearch的安装


 vi /etc/profile

在文件最后添加
export JAVA_HOME=/home/elk1/jdk1.8/jdk1.8.0_171
export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=.:$JAVA_HOME/LIB:$JRE_HOME/LIB:$CLASSPATH
export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH

[root@localhost jdk1.8.0_171]# source /etc/profile
[root@localhost jdk1.8.0_171]# java -version


ElasticSerach单机安装
[root@localhost elasticserach]# tar -zxvf elasticsearch-6.3.1.tar.gz
[root@localhost elasticserach]# cd elasticsearch-6.3.1/bin
[root@localhost bin]# ./elasticsearch
[root@localhost bin]# chown -R elk1:elk1 /home/elk1/elasticsearch

[elk1@localhost bin]$ ./elasticsearch

[elk1@localhost config]$ vi jvm.options


[root@localhost bin]vi config/elasticsearch.yml
network.host: 192.168.57.128



 vi /etc/security/limits.conf
 
 * hard nofile 65536
* soft nofile 131072
* hard nproc 4096
* soft nproc 2048


[3]	解决方案
[root@localhost bin]# vi /etc/sysctl.conf
[root@localhost bin]# sysctl -p

vm.max_map_count=655360
fs.file-max=655360


ElasticSerach集群安装
	修改配置文件elasticserach.yml
vim /elasticsearch.yml

cluster.name: aubin-cluster     #必须相同 
# 集群名称（不能重复）
node.name: els1（必须不同）
# 节点名称，仅仅是描述名称，用于在日志中区分（自定义）
#指定了该节点可能成为 master 节点，还可以是数据节点
	node.master: true
	node.data: true
path.data: /var/lib/elasticsearch
# 数据的默认存放路径（自定义）
path.logs: /var/log/elasticsearch 
# 日志的默认存放路径 
network.host: 192.168.0.1 
# 当前节点的IP地址 
http.port: 9200 
# 对外提供服务的端口
transport.tcp.port: 9300
#9300为集群服务的端口 
discovery.zen.ping.unicast.hosts: ["172.18.68.11", "172.18.68.12","172.18.68.13"] 
# 集群个节点IP地址，也可以使用域名，需要各节点能够解析 
discovery.zen.minimum_master_nodes: 2 
# 为了避免脑裂，集群节点数最少为 半数+1


注意：清空data和logs数据
192.168.14.12:9200/_cat/nodes?v

操作集群工具 
解压cerebro到bin下，打开cerebro.bat
web访问 http://localhost:9000/#/connect         添加集群的网址 http://192.168.57.128:9200

netstat -nltp | grep 5601









**************
logstash的安装
解压，配置

4.2、logstash安装      读取文件框架


[root@localhost logstash]# tar -zxvf logstash-6.3.1.tar.gz
mkdir  config 文件夹
[root@localhost logstash]# vi test.conf

input {
  stdin { }
}
output {
stdout {codec=>rubydebug}
}

[root@localhost logstash-6.3.1]# ./bin/logstash -f config/test.conf

命令echo "bar\n foo" | ../logstash-6.3.1/bin/logstash -f test.conf


file从文件读取数据  tail -f
配置：
path => [“/var/log/**/*.log”,”/var/log/message”]		文件位置
exclue => “*.gz”	不读取哪些文件
sincedb_path => “/var/log/message”	记录sincedb文件路径
start_postion => “beginning”		或者”end” 是否从头读取文件
stat_interval => 1000	单位秒，定时检查文件是否有更新，默认1S

执行：../logstash-6+3+1/bin/logstash -f ./test3.cof

input {
 file {
  path => ["/home/elk/logstsh/config/nginx_logs"]
  start_position => "beginning"
  type => "web"
 }
}

output {
 stdout {
  codec => "rubydebug"
 }
}


	Elasticsearch

input {
  elasticsearch {
    hosts => "192.168.14.10"
    index => "atguigu"
    query => '{ "query": { "match_all": {} }}'
  }
}

output {
 stdout {
  codec => "rubydebug"
 }
}
4.5、logstsh filter
Filter是logstsh功能强大的原因，它可以对数据进行丰富的处理，比如解析数据、删除字段、类型转换等
date：日期解析
grok：正则匹配解析
dissect：分割符解析
mutate：对字段作处理，比如重命名、删除、替换等
json：按照json解析字段内容到指定字段中
geoip：增加地理位置数据
ruby：利用ruby代码来动态修改logstsh Event

input {
  stdin {codec => “json”}
}

filter {
 date {
  match => ["logdate","MM dd yyyy HH:mm:ss"]
 }
}
output {
 stdout {
  codec => "rubydebug"
 }
}

{“logdate”:”Jan 01 2018 12:02:08”}

	Grok
正则匹配
%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] “%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}” %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}


input {
  http {port => 7474}
}

filter {
 grok {
  match => {
   "message" => "%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] “%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}” %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}"
  }
 }
}
output {
 stdout {
  codec => "rubydebug"
 }
}

93.180.71.3 - - [17/May/2015:08:05:32 +0000] "GET /downloads/product_1 HTTP/1.1" 304 0 "-" "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)"
93.180.71.3 - - [17/May/2015:08:05:23 +0000] "GET /downloads/product_1 HTTP/1.1" 304 0 "-" "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)"

	Logstsh ouput
stdout 
file :
file {
path => “/var/log/web.log”
codec => line {format => “%{message}”}
}
elasticsearch :
elasticsearch {
                hosts => ["http://192.168.14.10:9200"]
                index => "logstash-%{type}-%{+YYYY.MM.dd}"
        }






***************
kibana安装
上传解压，配置文件，连接到es

vim kibana.yml

sercer.port: 5601
server.host : "192+168+57+128"
elasticsearch.url: "http://192.168.57.128:9200"
kibana.index: ".kibana"








1 nginx安装环境
yum install gcc-c++ 
yum install -y pcre pcre-devel
yum install -y zlib zlib-devel
yum install -y openssl openssl-devel
将临时文件目录指定为/var/temp/nginx，需要在/var下创建temp及nginx目录
./configure \
--prefix=/usr/local/nginx \
--pid-path=/var/run/nginx/nginx.pid \
--lock-path=/var/lock/nginx.lock \
--error-log-path=/var/log/nginx/error.log \
--http-log-path=/var/log/nginx/access.log \
--with-http_gzip_static_module \
--http-client-body-temp-path=/var/temp/nginx/client \
--http-proxy-temp-path=/var/temp/nginx/proxy \
--http-fastcgi-temp-path=/var/temp/nginx/fastcgi \
--http-uwsgi-temp-path=/var/temp/nginx/uwsgi \
--http-scgi-temp-path=/var/temp/nginx/scgi


/var/log/nginx/access.log

nginx-1.15.1] make   和 make  install