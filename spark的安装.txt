spark安装

Spark安装地址
1．官网地址
http://spark.apache.org/
2．文档查看地址
https://spark.apache.org/docs/2.1.1/
3．下载地址
https://spark.apache.org/downloads.html

一 local模式：
        1）上传并解压spark安装包
            hadoop102 sorfware]$ tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/
            hadoop102 module]$ mv spark-2.1.1-bin-hadoop2.7 spark
        2）官方求PI案例
            [atguigu@hadoop102 spark]$ bin/spark-submit \
            --class org.apache.spark.examples.SparkPi \
            --executor-memory 1G \
            --total-executor-cores 2 \
            ./examples/jars/spark-examples_2.11-2.1.1.jar \
            100
        3） sc.textFile("input").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
访问hadoop101:4040     进程 SparkSubmit


集群模式安装：
    1、cd spark/conf/
    2、mv slaves.template slaves
    3、mv spark-env.sh.template spark-env.sh
    4、 vim slaves   
                    hadoop102
                    hadoop103
                    hadoop104
    5、vim spark-env.sh  
                    SPARK_MASTER_HOST=hadoop102
                    SPARK_MASTER_PORT=7077
                    
    6、vim spark-env.sh       
                    export JAVA_HOME=/opt/module/jdk1.8.0_144
    7、xsync spark/ 
    8、sbin/start-all.sh   
                        3330 Jps
                        3238 Worker
                        3163 Master
    9、运行 
                        bin/spark-submit \
                        --class org.apache.spark.examples.SparkPi \
                        --master spark://hadoop102:7077 \
                        --executor-memory 1G \
                        --total-executor-cores 2 \
                        ./examples/jars/spark-examples_2.11-2.1.1.jar \
                        100
    10、启动sparkshell
                /opt/module/spark/bin/spark-shell \
                --master spark://hadoop102:7077 \
                --executor-memory 1g \
                --total-executor-cores 2
        执行：sc.textFile("input").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
        
    11、配置历史服务器 JobHistoryServer配置
         mv spark-defaults.conf.template spark-defaults.conf
         vi spark-defaults.conf
         
                            spark.eventLog.enabled           true
                            spark.eventLog.dir               hdfs://hadoop102:9000/directory
                
         vi spark-env.sh
                             export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080
                            -Dspark.history.retainedApplications=30 
                            -Dspark.history.fs.logDirectory=hdfs://hadoop101:9000/directory"

      启动hadoop  创建文件夹directory
     查看网页  hadoop102:8080    http://hadoop101:18080/
     
     
     *************************
    sparkHA
1、vi spark-env.sh
                    注释掉如下内容：
                    #SPARK_MASTER_HOST=hadoop102
                    #SPARK_MASTER_PORT=7077
                    添加上如下内容：
                    export SPARK_DAEMON_JAVA_OPTS="
                    -Dspark.deploy.recoveryMode=ZOOKEEPER 
                    -Dspark.deploy.zookeeper.url=hadoop101,hadoop102,hadoop103 
                    -Dspark.deploy.zookeeper.dir=/spark"
2、xsync spark-env.sh
3、启动zookeeper
4、sbin/start-all.sh
5、hadoop103 spark]$ sbin/start-master.sh

********************spark-yarn 安装******************

   1）修改hadoop配置文件yarn-site.xml,添加如下内容：  vi yarn-site.xml 
                <!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
                    <property>
                            <name>yarn.nodemanager.pmem-check-enabled</name>
                            <value>false</value>
                    </property>
                    <!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
                    <property>
                            <name>yarn.nodemanager.vmem-check-enabled</name>
                            <value>false</value>
                    </property>
 
2）修改spark-env.sh，添加如下配置：   vi spark-env.sh

                        YARN_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop
                        
3)  xsync /opt/module/hadoop-2.7.2/etc/hadoop/yarn-site.xml
4)启动hdfs  yarn  
5）执行程序      bin/spark-submit \
            --class org.apache.spark.examples.SparkPi \
            --master yarn \
            --deploy-mode client \
            ./examples/jars/spark-examples_2.11-2.1.1.jar \
            100

            
            **********日志查看****************************
1、配置conf/spark-env.sh

                YARN_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop
                export JAVA_HOME=/opt/module/jdk1.8.0_144
                export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://hadoop101:9000/directory"
2  conf]$ vim spark-defaults.conf 
            
                spark.yarn.historyServer.address=hadoop101:18080
                spark.history.ui.port=18080
                spark.eventLog.dir      hdfs://hadoop101:9000/directory
                spark.eventLog.enabled  true
3访问
http://hadoop102:8088/cluster
http://hadoop101:18080/
http://hadoop103:19888/jobhistory


spark-yarn运行程序
bin/spark-submit --class com.atguigu.sparkdemo.WordCount --master spark://hadoop101:7077 WordCount.jar /input/input.txt /outspark
